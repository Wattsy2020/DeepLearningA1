{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wattsy2020/DeepLearningA1/blob/main/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysp-wjTD3eZl"
      },
      "source": [
        "# COMP5329 Assignment 1\n",
        "\n",
        "Team Member\n",
        "- Mirope Yuhao Hu, SID: 470139936\n",
        "- Sean Hongbo Du,\tSID: 500635346\n",
        "- Liam Watts, SID: 510562348\n",
        "\n",
        "\n",
        "### Liam Watts: 1, 2, 6 - More than 1 hidden layer, ReLU Activation, Softmax output layer and cross entroppy loss\n",
        "### Mirope: 4, 3, 8 - Momentum in SGD, weight decay, batch normalization\n",
        "### Sean: 5, 7, 0 - dropout, mini-batch training, data preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gYSdYbi_M5E"
      },
      "source": [
        "## How to run the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhWOqtJS3lah"
      },
      "source": [
        "## All necessary Imports\n",
        "\n",
        "__NOT__ allowed:\n",
        "1. use Deep Learning frameworks (e.g. PyTorch, Tensorflow, Caffe, and KERAS)\n",
        "2. any kinds of auto-grad tools (e.g. autograd)\n",
        "3. sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmsEjtP51rqd"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyOyluv169_Y"
      },
      "source": [
        "## Data Loading/Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bThU_z_hEFwL"
      },
      "source": [
        "def oneHot_encode():\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KbOQ3W4Ehzr",
        "outputId": "62ea6cbe-1d56-48f0-9044-ef487262522d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpxvRg-h7Cj1",
        "outputId": "714798aa-779f-4d2e-b53c-ae01ad2fffab"
      },
      "source": [
        "# load data -> subject to change for the final version\n",
        "dir_name = '/content/drive/MyDrive/Colab Notebooks/COMP5329/Ass1/Assignment1-Dataset/'\n",
        "\n",
        "test_data = np.load(dir_name + 'test_data.npy')\n",
        "test_label = np.load(dir_name + 'test_label.npy')\n",
        "train_data = np.load(dir_name + 'train_data.npy')\n",
        "train_label = np.load(dir_name + 'train_label.npy')\n",
        "\n",
        "print('Shape of original train data is:\\t', train_data.shape,\n",
        "      '\\nShape of original train label is:\\t', train_label.shape,\n",
        "      '\\nShape of original test data is:\\t\\t', test_data.shape,\n",
        "      '\\nShape of original test label is:\\t', test_label.shape )\n",
        "\n",
        "# print('\\nAll labels are:\\t\\t', list(np.unique(test_label)))\n",
        "\n",
        "# # preprocessing: normalization might be unnecessary\n",
        "# # test_x = Normalizer().fit_transform(test_data)\n",
        "# # train_x = Normalizer().fit_transform(train_data)\n",
        "# test_x = test_data\n",
        "# train_x = train_data\n",
        "# test_y = OneHotEncoder().fit_transform(test_label).toarray()\n",
        "# train_y = OneHotEncoder().fit_transform(train_label).toarray()\n",
        "\n",
        "# print('\\nShape of transformed train label is:\\t', train_y.shape,\n",
        "#       '\\nShape of transformed train label is:\\t', test_y.shape )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of original train data is:\t (50000, 128) \n",
            "Shape of original train label is:\t (50000, 1) \n",
            "Shape of original test data is:\t\t (10000, 128) \n",
            "Shape of original test label is:\t (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouDN5SF37EWa"
      },
      "source": [
        "## Algorithms / Structures\n",
        "\n",
        "__Requirements__\n",
        "1. More than one hidden layer\n",
        "2. ReLU activation\n",
        "3. Weight decay\n",
        "4. Momentum in SGD\n",
        "5. Dropout\n",
        "6. Softmax and cross-entropy \n",
        "7. Mini-batch training\n",
        "8. Batch Normalization\n",
        "\n",
        "In this part, try to implement each layer/optimiser/function as modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGKGWUZ4_WV1"
      },
      "source": [
        "# Liam Watts: 1, 2, 6 - activation, layer, softmax\n",
        "# Mirope: 4, 3, 8 - Momentum in SGD, weight decay, batch normalization\n",
        "# Sean: 5, 7, 0 - dropout, mini-batch training, data preprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBv_AQN9xXak",
        "outputId": "be3984c8-a38b-45ce-b919-db2239729c48"
      },
      "source": [
        "dlogsigmoid(sigmoid(np.array([[1, 2, 3], [4, 5, 6]])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.26894142, 0.11920292, 0.04742587],\n",
              "       [0.01798621, 0.00669285, 0.00247262]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpiFz0wdSCzU"
      },
      "source": [
        "# Define functions required for the Multi Layer Perceptron\n",
        "def he_uniform(input_size=1, output_size=1):\n",
        "    '''outputs a matrix of size (output_size, input_size) with random initialization'''\n",
        "    r = np.sqrt(6/input_size)\n",
        "    return np.random.uniform(-r, r, size=(output_size, input_size))\n",
        "\n",
        "def relu(matrix):\n",
        "    '''return the elementwise ReLU activation of the matrix'''\n",
        "    return np.maximum(matrix, 0)\n",
        "\n",
        "def drelu(matrix):\n",
        "    '''\n",
        "    parameters:\n",
        "        matrix: the relu activations\n",
        "    \n",
        "    outputs:\n",
        "        the derivative for relu on every element of the matrix\n",
        "    '''\n",
        "    return np.minimum(np.maximum(matrix, 0), 1) # this defines the derivative of relu at 0 as 0 to tf.nn.relu()'s standard (which ensures a sparser gradient matrix)\n",
        "\n",
        "# define the sigmoid function for testing, it's easier to implement backprop on sigmoid than softmax\n",
        "def sigmoid(matrix):\n",
        "    '''return sigmoid on every element of the matrix'''\n",
        "    return 1/(1 + np.power(np.e, -matrix))\n",
        "\n",
        "def dlogsigmoid(matrix):\n",
        "    '''\n",
        "    parameters:\n",
        "        matrix: the sigmoid activations\n",
        "    \n",
        "    outputs:\n",
        "        the derivative for log sigmoid on every element of the matrix\n",
        "    '''\n",
        "    return (1 - matrix) # dsigmoid = sigmoid(1 - sigmoid)  so dlog(sigmoid) = sigmoid(1 - sigmoid)/sigmoid = (1 - sigmoid)\n",
        "\n",
        "def softmax(matrix):\n",
        "    '''return the softmax activation of a matrix (each row vector has softmax applied to it)'''\n",
        "    matrix = np.power(np.e, matrix)\n",
        "    total = np.sum(matrix, axis=1).reshape(matrix.shape[0], 1)\n",
        "    return matrix/total # here we broadcast total (a vector) to divide each row in the matrix, broadcasting is frequently used to vectorize our code\n",
        "\n",
        "# it is faster to calculate the derivative of log(softmax(vector)), and because we use cross entropy as the loss function\n",
        "# the only derivative we need to calculate is this log(softmax(vector))\n",
        "def dlogsoftmax(vector):\n",
        "    '''\n",
        "    parameters:\n",
        "        vector: the softmax activations\n",
        "    \n",
        "    outputs:\n",
        "        the jacobian, a matrix of derivatives of log(vector[i]) wrt vector[j]\n",
        "    '''\n",
        "    pass # to implement (examine this https://stackoverflow.com/questions/35304393/trying-to-understand-code-that-computes-the-gradient-wrt-to-the-input-for-logsof)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYQ1P2lNP9RL"
      },
      "source": [
        "class MLP:\n",
        "    ''' \n",
        "    Implements a Multi Layer Perceptron, including both the feed-forward phase and backward propagation\n",
        "    The output activation must be softmax, and the loss function cross entropy\n",
        "\n",
        "    MLP.__init__()\n",
        "        parameters:\n",
        "            hidden: an array with the number of neurons for each layer\n",
        "        \n",
        "        initializes weight matrices with shape (output_size, input_size) to match Pytorch's standard\n",
        "    '''\n",
        "    def __init__(self, hidden=[1], input_size=1, output_size=1, initializer=he_uniform):\n",
        "        '''initialize the weight and bias matrices for each layer'''\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        for input_size_i, output_size_i in zip([input_size] + hidden, hidden + [output_size]):\n",
        "            self.weights.append(initializer(input_size=input_size_i, output_size=output_size_i))\n",
        "            self.biases.append(np.zeros((output_size_i, 1)))\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        calculate output of the network, storing the hidden states in self.activations (activations[0] is the feature input)\n",
        "        \n",
        "        parameters:\n",
        "            X: the input matrix of shape (batch_size, input_size)\n",
        "\n",
        "        returns:\n",
        "            yhat: the prediction matrix of shape (batch_size, output_size)\n",
        "        '''\n",
        "        self.activations = [X]\n",
        "        for w, b in zip(self.weights, self.biases):\n",
        "            activation = self.forward_one_layer(self.activations[-1], w, b)\n",
        "            self.activations.append(activation)\n",
        "        return softmax(self.activations[-1])\n",
        "\n",
        "    def forward_one_layer(self, inputs, weights, biases):\n",
        "        '''\n",
        "        compute the output of a layer for certain inputs, return the output\n",
        "        \n",
        "        we follow the pytorch standard, so shapes are as follows:\n",
        "        inputs: a matrix of shape (batch_size, feature_size)\n",
        "        weights: a matrix of shape (output_size, feature_size)\n",
        "        biases: a matrix of shape (output_size, 1)\n",
        "        returns: a matrix of shape (batch_size, output_size)\n",
        "        '''\n",
        "        return relu(np.dot(inputs, weights.T) + biases.T)\n",
        "\n",
        "    def backprop(self, y):\n",
        "        '''given the true outputs y perform one step of gradient descent'''\n",
        "        assert self.hidden != [] # we need the hidden states\n",
        "        # get the derivative of cross-entropy loss for the output\n",
        "\n",
        "        # do backprop layer by layer, passing to each layer the derivative of loss w.r.t that layer's outputs\n",
        "            # while simultaneously updating the weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR6rcOgXReNC",
        "outputId": "68e16c97-6b18-4805-eb0a-de5dccd8cbb2"
      },
      "source": [
        "mlp = MLP(hidden=[64], input_size=128, output_size=10)\n",
        "preds = mlp.forward(train_data)\n",
        "preds, np.sum(preds, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.07955625, 0.05938695, 0.12384888, ..., 0.13039277, 0.05938695,\n",
              "         0.05938695],\n",
              "        [0.08884543, 0.11836953, 0.04426743, ..., 0.1514843 , 0.07748669,\n",
              "         0.04426743],\n",
              "        [0.02641715, 0.66916231, 0.03463996, ..., 0.02641715, 0.04047708,\n",
              "         0.05971798],\n",
              "        ...,\n",
              "        [0.01672059, 0.8305105 , 0.01672059, ..., 0.01672059, 0.01672059,\n",
              "         0.01707037],\n",
              "        [0.0114723 , 0.60479851, 0.0116819 , ..., 0.00958395, 0.20692705,\n",
              "         0.03454694],\n",
              "        [0.15077616, 0.08481901, 0.0352657 , ..., 0.03517824, 0.39013562,\n",
              "         0.15460947]]), array([1., 1., 1., ..., 1., 1., 1.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3keVMY5R8OFu"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGxDw6Fe8QbT"
      },
      "source": [
        "# the baseline model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=128, activation='relu')) # 2\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.1)) # 5\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.1)) # 5\n",
        "    model.add(BatchNormalization()) # 8\n",
        "    model.add(Dense(10, activation='softmax')) # 1, 6.1\n",
        "    opt = SGD(lr=0.3, momentum=0.9, decay=0.001) # optimizers: 3, 4\n",
        "    # 6.2\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  \n",
        "# Crossing Validation\n",
        "def training(model, epoches=20, batch_size=256):\n",
        "  for i in epoches:\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}