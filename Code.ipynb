{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wattsy2020/DeepLearningA1/blob/main/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysp-wjTD3eZl"
      },
      "source": [
        "# COMP5329 Assignment 1\n",
        "\n",
        "Team Member\n",
        "- Mirope Yuhao Hu, SID: 470139936\n",
        "- Sean Hongbo Du,\tSID: 500635346\n",
        "- Liam Watts, SID: 510562348\n",
        "\n",
        "\n",
        "### Liam Watts: 1, 2, 6 - More than 1 hidden layer, ReLU Activation, Softmax output layer and cross entroppy loss\n",
        "### Mirope: 4, 3, 8 - Momentum in SGD, weight decay, batch normalization\n",
        "### Sean: 5, 7, 0 - dropout, mini-batch training, data preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gYSdYbi_M5E"
      },
      "source": [
        "## How to run the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhWOqtJS3lah"
      },
      "source": [
        "## All necessary Imports\n",
        "\n",
        "__NOT__ allowed:\n",
        "1. use Deep Learning frameworks (e.g. PyTorch, Tensorflow, Caffe, and KERAS)\n",
        "2. any kinds of auto-grad tools (e.g. autograd)\n",
        "3. sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmsEjtP51rqd"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyOyluv169_Y"
      },
      "source": [
        "## Data Loading/Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bThU_z_hEFwL"
      },
      "source": [
        "def oneHot_encode():\n",
        "    pass"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KbOQ3W4Ehzr",
        "outputId": "696af2f5-bca2-4191-8709-37f9fc1f9e00"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpxvRg-h7Cj1",
        "outputId": "30c4f603-fbef-43d4-b436-518dcf4e3054"
      },
      "source": [
        "# load data -> subject to change for the final version\n",
        "dir_name = '/content/drive/MyDrive/Colab Notebooks/COMP5329/Ass1/Assignment1-Dataset/'\n",
        "\n",
        "test_data = np.load(dir_name + 'test_data.npy')\n",
        "test_label = np.load(dir_name + 'test_label.npy')\n",
        "train_data = np.load(dir_name + 'train_data.npy')\n",
        "train_label = np.load(dir_name + 'train_label.npy')\n",
        "\n",
        "print('Shape of original train data is:\\t', train_data.shape,\n",
        "      '\\nShape of original train label is:\\t', train_label.shape,\n",
        "      '\\nShape of original test data is:\\t\\t', test_data.shape,\n",
        "      '\\nShape of original test label is:\\t', test_label.shape )\n",
        "\n",
        "# print('\\nAll labels are:\\t\\t', list(np.unique(test_label)))\n",
        "\n",
        "# # preprocessing: normalization might be unnecessary\n",
        "# # test_x = Normalizer().fit_transform(test_data)\n",
        "# # train_x = Normalizer().fit_transform(train_data)\n",
        "# test_x = test_data\n",
        "# train_x = train_data\n",
        "# test_y = OneHotEncoder().fit_transform(test_label).toarray()\n",
        "# train_y = OneHotEncoder().fit_transform(train_label).toarray()\n",
        "\n",
        "# print('\\nShape of transformed train label is:\\t', train_y.shape,\n",
        "#       '\\nShape of transformed train label is:\\t', test_y.shape )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of original train data is:\t (50000, 128) \n",
            "Shape of original train label is:\t (50000, 1) \n",
            "Shape of original test data is:\t\t (10000, 128) \n",
            "Shape of original test label is:\t (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouDN5SF37EWa"
      },
      "source": [
        "## Algorithms / Structures\n",
        "\n",
        "__Requirements__\n",
        "1. More than one hidden layer\n",
        "2. ReLU activation\n",
        "3. Weight decay\n",
        "4. Momentum in SGD\n",
        "5. Dropout\n",
        "6. Softmax and cross-entropy \n",
        "7. Mini-batch training\n",
        "8. Batch Normalization\n",
        "\n",
        "In this part, try to implement each layer/optimiser/function as modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGKGWUZ4_WV1"
      },
      "source": [
        "# Liam Watts: 1, 2, 6 - activation, layer, softmax\n",
        "# Mirope: 4, 3, 8 - Momentum in SGD, weight decay, batch normalization\n",
        "# Sean: 5, 7, 0 - dropout, mini-batch training, data preprocess"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpiFz0wdSCzU"
      },
      "source": [
        "# Define functions required for the Multi Layer Perceptron\n",
        "def he_uniform(input_size=1, output_size=1):\n",
        "    '''outputs a matrix of size (output_size, input_size) with random initialization'''\n",
        "    r = np.sqrt(6/input_size)\n",
        "    return np.random.uniform(-r, r, size=(output_size, input_size))\n",
        "\n",
        "def relu(matrix):\n",
        "    '''return the elementwise ReLU activation of the matrix'''\n",
        "    return np.maximum(matrix, 0)\n",
        "\n",
        "def drelu(matrix):\n",
        "    '''\n",
        "    parameters:\n",
        "        matrix: the relu activations\n",
        "    \n",
        "    outputs:\n",
        "        the derivative for relu on every element of the matrix\n",
        "    '''\n",
        "    return np.minimum(np.maximum(matrix, 0), 1) # this defines the derivative of relu at 0 as 0 to tf.nn.relu()'s standard (which ensures a sparser gradient matrix)\n",
        "\n",
        "# define the sigmoid function for testing, it's easier to implement backprop on sigmoid than softmax\n",
        "def sigmoid(matrix):\n",
        "    '''return sigmoid on every element of the matrix'''\n",
        "    return 1/(1 + np.power(np.e, -matrix))\n",
        "\n",
        "def dlogsigmoid(matrix):\n",
        "    '''\n",
        "    parameters:\n",
        "        matrix: the sigmoid activations\n",
        "    \n",
        "    outputs:\n",
        "        the derivative for log sigmoid on every element of the matrix\n",
        "    '''\n",
        "    return (1 - matrix) # dsigmoid = sigmoid(1 - sigmoid)  so dlog(sigmoid) = sigmoid(1 - sigmoid)/sigmoid = (1 - sigmoid)\n",
        "\n",
        "def softmax(matrix):\n",
        "    '''return the softmax activation of a matrix (each row vector has softmax applied to it)'''\n",
        "    matrix = np.power(np.e, matrix)\n",
        "    total = np.sum(matrix, axis=1).reshape(matrix.shape[0], 1)\n",
        "    return matrix/total # here we broadcast total (a vector) to divide each row in the matrix, broadcasting is frequently used to vectorize our code\n",
        "\n",
        "# it is faster to calculate the derivative of log(softmax(vector)), and because we use cross entropy as the loss function\n",
        "# the only derivative we need to calculate is this log(softmax(vector))\n",
        "def dlogsoftmax(vector):\n",
        "    '''\n",
        "    parameters:\n",
        "        vector: the softmax activations\n",
        "    \n",
        "    outputs:\n",
        "        the jacobian, a matrix of derivatives of log(vector[i]) wrt vector[j]\n",
        "    '''\n",
        "    pass # to implement (examine this https://stackoverflow.com/questions/35304393/trying-to-understand-code-that-computes-the-gradient-wrt-to-the-input-for-logsof)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYQ1P2lNP9RL"
      },
      "source": [
        "class MLP:\n",
        "    ''' \n",
        "    Implements a Multi Layer Perceptron, including both the feed-forward phase and backward propagation\n",
        "    The output activation must be softmax, and the loss function cross entropy\n",
        "\n",
        "    MLP.__init__()\n",
        "        parameters:\n",
        "            hidden: an array with the number of neurons for each layer\n",
        "        \n",
        "        initializes weight matrices with shape (output_size, input_size) to match Pytorch's standard\n",
        "    '''\n",
        "    def __init__(self, hidden=[1], input_size=1, output_size=1, learn_rate=0.01, initializer=he_uniform):\n",
        "        '''initialize the weight and bias matrices for each layer, along with other hyperparameters'''\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        for input_size_i, output_size_i in zip([input_size] + hidden, hidden + [output_size]):\n",
        "            self.weights.append(initializer(input_size=input_size_i, output_size=output_size_i))\n",
        "            self.biases.append(np.zeros((output_size_i, 1)))\n",
        "\n",
        "        self.learn_rate=learn_rate\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        calculate output of the network, storing the hidden states in self.activations (activations[0] is the feature input)\n",
        "        \n",
        "        parameters:\n",
        "            X: the input matrix of shape (batch_size, input_size)\n",
        "\n",
        "        returns:\n",
        "            yhat: the prediction matrix of shape (batch_size, output_size)\n",
        "        '''\n",
        "        self.activations = [X]\n",
        "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            if i == len(self.weights) - 1: # use sigmoid activation in the last layer. TODO: change to softmax\n",
        "                activation = self.forward_one_layer(self.activations[i], w, b, activation=sigmoid)\n",
        "                #print(\"activation: \", activation[0, :])\n",
        "            else:\n",
        "                activation = self.forward_one_layer(self.activations[i], w, b, activation=relu)\n",
        "            self.activations.append(activation)\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def forward_one_layer(self, inputs, weights, biases, activation=relu):\n",
        "        '''\n",
        "        compute the output of a layer for certain inputs, return the output\n",
        "        \n",
        "        we follow the pytorch standard, so shapes are as follows:\n",
        "        inputs: a matrix of shape (batch_size, feature_size)\n",
        "        weights: a matrix of shape (output_size, feature_size)\n",
        "        biases: a matrix of shape (output_size, 1)\n",
        "        returns: a matrix of shape (batch_size, output_size)\n",
        "        '''\n",
        "        linear = np.dot(inputs, weights.T) + biases.T\n",
        "        return activation(linear)\n",
        "\n",
        "    def backprop(self, y, yhat):\n",
        "        '''given the true outputs y and predictions yhat, perform one step of gradient descent'''\n",
        "        assert self.activations != [], \"forward propagate first before back propagation\" # we need the hidden states\n",
        "        # get the derivative of cross-entropy loss for the output\n",
        "        dloss = yhat - y # TODO: use softmax log loss instead\n",
        "\n",
        "        # do backprop layer by layer, passing to each layer the derivative of loss w.r.t that layer's outputs\n",
        "        for layer in range(len(self.weights) - 1, -1, -1):\n",
        "            dloss = self.backprop_one_layer(dloss, layer)\n",
        "        self.activations = [] # reset the activations to avoid using them twice for the same backprop\n",
        "\n",
        "    def backprop_one_layer(self, dloss, i):\n",
        "        '''\n",
        "        backpropagate one layer of the model\n",
        "        parameters:\n",
        "            i: the index of the layer to propagate in self.weights and self.biases\n",
        "            dloss: loss of the output (pre relu) of this layer, shape of (batch_size, layer_output_size)\n",
        "        \n",
        "        updates the weights and biases by subtracting the gradient\n",
        "\n",
        "        returns:\n",
        "            dloss: loss of the output (pre relu) of the previous layer (layer i-1), so long as layer i-1 exists (i.e. i-1 >= 0)\n",
        "        '''\n",
        "        assert dloss is not None, \"Output deltas not provided\"\n",
        "        \n",
        "        # outline dimensions\n",
        "        batch_size = dloss.shape[0]\n",
        "        layer_output_size = dloss.shape[1]\n",
        "        layer_input_size = self.activations[i].shape[1]\n",
        "        \n",
        "        # calculate gradients\n",
        "        activation = self.activations[i].reshape(batch_size, layer_input_size) # use reshapes to assert the shape is correct, and indicate shape\n",
        "        dw = np.dot(dloss.T, activation).reshape(layer_output_size, layer_input_size)\n",
        "                                         # the i,j entry of dw represents the dot product of:\n",
        "                                         # the ith row of dloss.T i.e. the derivative of losses for neuron i over all samples\n",
        "                                         # the jth column of activation i.e. the input to weight j over all samples\n",
        "                                         # so overall: it is (dloss for neuron i * input for weight j of neuron i), giving us the derivative of loss w.r.t weight i,j\n",
        "        db = np.sum(dloss.T, axis=1).reshape(layer_output_size, 1)\n",
        "                                     # output = X*W.T + B so doutput w.r.t db is just 1, hence we only need to sum the gradients for each bias\n",
        "        \n",
        "        # calculate gradient of loss w.r.t output of previous layer (if there is a previous layer)\n",
        "        if i != 0:\n",
        "            dloss_prev = np.dot(dloss, self.weights[i]).reshape(batch_size, layer_input_size)\n",
        "                                          # the i,j entry of dloss_prev represents the dot product of\n",
        "                                          # the ith row of dloss i.e. the dlosses for the ith sample for each of this layers outputs\n",
        "                                          # the jth column of W i.e. the jth weight for all output neurons in this layer\n",
        "                                          # overall: the sum of (dloss of output neuron k * weight j (the weight that multiplies input j) of output neuron k for sample i)\n",
        "                                          # which is the derivative of the loss w.r.t input j for this sample as output = X*W.T + B\n",
        "            dloss_prev = dloss_prev * drelu(activation) # multiply by the derivatives of the previous layers activations\n",
        "        else:\n",
        "            dloss_prev = None\n",
        "\n",
        "        # update gradients\n",
        "        self.weights[i] -= (self.learn_rate/batch_size)*dw\n",
        "        self.biases[i] -= (self.learn_rate/batch_size)*db\n",
        "\n",
        "        return dloss_prev"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3KfPIbAf3DK",
        "outputId": "c670b7c8-97d4-433c-ecbc-9cd6a349ae01"
      },
      "source": [
        "# test sigmoid MLP training\n",
        "# create a dataset for training\n",
        "X = np.random.rand(10000, 4)\n",
        "#y = (np.sum(X, axis=1) > 2) # the task is to identify if the sum of 4 numbers is greater than 2\n",
        "y = X[:, 0] > 0.5\n",
        "y = y.reshape(10000, 1)\n",
        "np.concatenate([X, y], axis=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.70971506, 0.40725527, 0.16487558, 0.79116544, 1.        ],\n",
              "       [0.55246974, 0.14443199, 0.52638744, 0.01795876, 1.        ],\n",
              "       [0.30822828, 0.28431978, 0.91892987, 0.54763407, 0.        ],\n",
              "       ...,\n",
              "       [0.68330851, 0.75867031, 0.70576458, 0.79125109, 1.        ],\n",
              "       [0.6212175 , 0.56333251, 0.73478416, 0.2529044 , 1.        ],\n",
              "       [0.28774412, 0.23783147, 0.38273565, 0.75358385, 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJTydtMkRNcv",
        "outputId": "8eae7f56-0e15-4bae-a53d-baa833120112"
      },
      "source": [
        "mlp = MLP(hidden=[100, 50, 25], input_size=4, output_size=1, learn_rate=0.01)\n",
        "\n",
        "# get accuracy function\n",
        "def accuracy(mlp, X, y):\n",
        "    preds = mlp.forward(X) > 0.5\n",
        "    accuracy = np.mean(preds == y)\n",
        "    return accuracy\n",
        "\n",
        "# train model\n",
        "batch_size = 100\n",
        "for epoch in range(100):\n",
        "    # shuffle\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X = X[indices, :]\n",
        "    y = y[indices, :]\n",
        "\n",
        "    for batch in range(int(X.shape[0]/batch_size)):\n",
        "        batch_x = X[batch*batch_size:(batch+1)*batch_size]\n",
        "        batch_y = y[batch*batch_size:(batch+1)*batch_size]\n",
        "        preds = mlp.forward(batch_x)\n",
        "        mlp.backprop(batch_y, preds)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch {}    Accuracy: {}\".format(epoch, accuracy(mlp, X, y)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0    Accuracy: 0.6262\n",
            "Epoch 10    Accuracy: 0.936\n",
            "Epoch 20    Accuracy: 0.9646\n",
            "Epoch 30    Accuracy: 0.977\n",
            "Epoch 40    Accuracy: 0.984\n",
            "Epoch 50    Accuracy: 0.9886\n",
            "Epoch 60    Accuracy: 0.9903\n",
            "Epoch 70    Accuracy: 0.9932\n",
            "Epoch 80    Accuracy: 0.9948\n",
            "Epoch 90    Accuracy: 0.994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdunAZGcrk2G",
        "outputId": "22d5cc07-4d12-4368-8b29-00a7b19be42a"
      },
      "source": [
        "mlp.weights[-1], mlp.biases[-1]"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[15.29173464, -0.27511754, -0.20489533, -0.22305846]]),\n",
              " array([[-7.22438466]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR6rcOgXReNC",
        "outputId": "9ce7faf9-09cf-4665-f81a-234e9608c65e"
      },
      "source": [
        "mlp = MLP(hidden=[64], input_size=128, output_size=10)\n",
        "preds = mlp.forward(train_data)\n",
        "preds, np.sum(preds, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.03418064, 0.03418064, 0.27683855, ..., 0.03959415, 0.05997034,\n",
              "         0.28327345],\n",
              "        [0.07543711, 0.03333939, 0.03333939, ..., 0.03333939, 0.03333939,\n",
              "         0.1027464 ],\n",
              "        [0.07403353, 0.07403353, 0.07403353, ..., 0.14066237, 0.07403353,\n",
              "         0.07403353],\n",
              "        ...,\n",
              "        [0.10015951, 0.11925923, 0.08579675, ..., 0.18000399, 0.08579675,\n",
              "         0.08579675],\n",
              "        [0.11504859, 0.01624576, 0.01624576, ..., 0.01624576, 0.01624576,\n",
              "         0.14411416],\n",
              "        [0.11432698, 0.08168115, 0.08168115, ..., 0.08168115, 0.08168115,\n",
              "         0.23222384]]), array([1., 1., 1., ..., 1., 1., 1.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3keVMY5R8OFu"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGxDw6Fe8QbT"
      },
      "source": [
        "# the baseline model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=128, activation='relu')) # 2\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.1)) # 5\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.1)) # 5\n",
        "    model.add(BatchNormalization()) # 8\n",
        "    model.add(Dense(10, activation='softmax')) # 1, 6.1\n",
        "    opt = SGD(lr=0.3, momentum=0.9, decay=0.001) # optimizers: 3, 4\n",
        "    # 6.2\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  \n",
        "# Crossing Validation\n",
        "def training(model, epoches=20, batch_size=256):\n",
        "  for i in epoches:\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}